{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30718e76-0333-4714-a864-d50ed661e418",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a powerful approach that combines retrieval-based methods with generative models to provide contextually relevant and informative answers. In this notebook, we use LangChain's ecosystem to set up a conversational RAG system that uses documents stored as embeddings for rapid retrieval and accurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e86c7-9741-4dea-9ce0-07ee4f6dcf1c",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2291cf43-28fe-480d-9aab-bfef9797cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b35add-bf27-491d-af9f-85d9df9d1547",
   "metadata": {},
   "source": [
    "## Allow nested asynchronous loops\n",
    "\n",
    "Jupyter notebooks already have an event loop running in the background, making it challenging to run asynchronous code directly. `nest_asyncio.apply()` resolves this by allowing asynchronous code to run within a notebook cell, even if the loop is already active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56acc7e4-5efc-49d9-96a8-827600b6810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740fe2e-4943-42fe-8558-93f08df83803",
   "metadata": {},
   "source": [
    "## The `get_conversational_answer` Function\n",
    "\n",
    "### Contextualizing the Question\n",
    "- The function starts by setting up a `contextualize_q_system_prompt`, which is a system instruction that reformulates the user's question based on the chat history. This step ensures that questions referencing past conversation context are rewritten as standalone questions that can be understood without that context.\n",
    "- The prompt is then fed into a `ChatPromptTemplate`, which organizes the messages for the language model. It includes placeholders for the system message, chat history, and user input.\n",
    "\n",
    "### Creating a History-Aware Retriever\n",
    "- `mistral:7b` is initialized as the LLM.\n",
    "- Using this LLM, `create_history_aware_retriever` is called, which combines the LLM with a retriever (a tool that fetches relevant documents). This retriever will be context-aware, ensuring conversational flow.\n",
    "\n",
    "### Setting Up the Question-Answering (QA) System Prompt\n",
    "- The `qa_system_prompt` is another system message that directs the assistant to answer the question concisely and to only respond if it has enough information.\n",
    "- A second `ChatPromptTemplate` is created to format these QA instructions, integrating context, chat history, and user input.\n",
    "\n",
    "### Creating the RAG Chain\n",
    "- A `question_answer_chain` is created using `create_stuff_documents_chain`, which combines the LLM and the QA prompt. This chain processes the retrieved documents (context) and provides answers.\n",
    "- Next, `create_retrieval_chain` links the history-aware retriever and the question-answering chain to form a RAG pipeline. The pipeline retrieves relevant context from documents and uses it to generate concise and precise answers.\n",
    "\n",
    "### Generating the Answer\n",
    "- The `rag_chain.invoke` method is called with the user input and chat history, returning a response (`ai_msg`) from the RAG pipeline. This response is structured to provide clear, contextually accurate answers based on both the user’s question and the retrieved documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf64f494-507f-416e-b55d-bfdd8d5b454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_conversational_answer(retriever, input, chat_history):\n",
    "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "    which might reference context in the chat history, formulate a standalone question \\\n",
    "    which can be understood without the chat history. Do NOT answer the question, \\\n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = ChatOllama(model=\"mistral:7b\")\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Use three sentences maximum and keep the answer concise.\\\n",
    "    Do not generate any additional text unless you are asked to.\\\n",
    "    Keep the answers really short and concise.\\\n",
    "\n",
    "    {context}\"\"\"\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    ai_msg = rag_chain.invoke({\"input\": input, \"chat_history\": chat_history})\n",
    "    return ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756bb32-3f1d-4d2e-91aa-66cd9ec6c25a",
   "metadata": {},
   "source": [
    "## The `main` Function \n",
    "\n",
    "The `main` function initiates the conversational question-answering chain based on PDF documents stored in the specified directory.\n",
    "\n",
    "### PDF Document Loading\n",
    "- The directory containing PDF files is specified (`./data`), and these documents are loaded using `PyPDFDirectoryLoader`.\n",
    "- `documents` stores the loaded PDF data, which will be converted into embeddings for retrieval.\n",
    "\n",
    "### Vector Store Initialization\n",
    "- `OllamaEmbeddings` (using the \"mistral:7b\" model) is used to generate embeddings for the documents, which allows for semantic similarity searching.\n",
    "- Facebook AI Similarity Search (FAISS) is an open-source library that helps developers search for similar multimedia documents in large datasets. It stores these document embeddings, enabling quick retrieval based on the user's questions.\n",
    "- The vector store’s `as_retriever()` method provides a retriever object for retrieving relevant document chunks.\n",
    "\n",
    "### Conversation State Initialization\n",
    "- `chat_history` is initialized as an empty list to store user inputs and assistant responses. This is later used for reformulating user questions to enable contextual question answering. \n",
    "\n",
    "### Interactive Question-Answer Loop\n",
    "- A loop takes user input (prompt) to ask questions based on the uploaded PDF documents.\n",
    "- The loop breaks if the user types \"exit\".\n",
    "\n",
    "### Getting the AI Response\n",
    "- The `get_conversational_answer` function is called using `asyncio.run()`, taking in the retriever, user prompt, and chat history to generate contextually relevant responses.\n",
    "- The AI’s answer (`ai_msg[\"answer\"]`) and the user’s question are added to `chat_history` for providing context in the future responses.\n",
    "\n",
    "### Displaying the Assistant’s Response\n",
    "- The assistant’s response is printed to the console.\n",
    "- This loop continues until the user types in 'exit'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b62c52b-aa2a-4b5a-b0c8-d3c188d2072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Specify the directory where the PDF is stored\n",
    "    pdf_directory = \"./data\"\n",
    "\n",
    "    # Load the PDF documents\n",
    "    loader = PyPDFDirectoryLoader(pdf_directory)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Initialize the vector store using the embeddings model\n",
    "    embed_model = OllamaEmbeddings(model='mistral:7b')\n",
    "    vector_store = FAISS.from_documents(documents, embed_model)\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    # Initialize the conversation state\n",
    "    chat_history = []\n",
    "\n",
    "    while True:\n",
    "        # Take user input for a question\n",
    "        prompt = input(\"Ask your question based on the uploaded PDF (or type 'exit' to quit): \")\n",
    "\n",
    "        if prompt.lower() == 'exit':\n",
    "            print(\"Exiting the conversation.\")\n",
    "            break\n",
    "\n",
    "        # Get the AI response using the retriever and chain\n",
    "        ai_msg = asyncio.run(get_conversational_answer(retriever, prompt, chat_history))\n",
    "\n",
    "        # Store the user input and AI response in the chat history\n",
    "        chat_history.extend([HumanMessage(content=prompt), ai_msg[\"answer\"]])\n",
    "\n",
    "        # Display the assistant's response\n",
    "        print(\"Assistant: \", ai_msg[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba129623-b95a-4a1a-947d-5dabbb355b84",
   "metadata": {},
   "source": [
    "## Call the `main` function to initiate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fa83809-81ca-439b-a1bd-046725b97dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question based on the uploaded PDF (or type 'exit' to quit):  what are the things to know about customers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12201/400163513.py:14: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"mistral:7b\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  1. Understand Their Needs: Customers purchase products or services based on their needs, desires, and problems they want to solve. Understanding these needs is crucial for providing them with a product or service that meets their requirements.\n",
      "\n",
      "2. Know Their Preferences: Every customer has unique preferences when it comes to products, services, and the way they are delivered. This can include everything from color choices, brand loyalty, payment methods, and more.\n",
      "\n",
      "3. Demographics: Basic demographic information such as age, gender, income level, education level, occupation, and location can provide valuable insights into customer behavior, preferences, and purchasing power.\n",
      "\n",
      "4. Psychographics: This refers to a customer's values, attitudes, interests, and lifestyle. Understanding these aspects can help businesses connect with customers on a deeper emotional level and tailor their marketing strategies accordingly.\n",
      "\n",
      "5. Customer Journey: Understanding the steps a customer takes from the initial awareness of your product or service to the final purchase is crucial for improving the overall customer experience and increasing conversions.\n",
      "\n",
      "6. Customer Lifetime Value (CLV): CLV is the total amount a business can reasonably expect to make from a single customer account throughout their relationship with the company. Knowing this value helps businesses prioritize customer retention efforts.\n",
      "\n",
      "7. Behavioral Patterns: Analyzing a customer's past behavior, such as purchase history, website visits, and interactions with your brand, can provide insights into their future behavior and help you anticipate their needs.\n",
      "\n",
      "8. Feedback: Regularly seeking and acting upon customer feedback is essential for improving products, services, and overall customer satisfaction. This can be done through surveys, reviews, social media, and in-person interactions.\n",
      "\n",
      "9. Segmentation: Grouping customers based on common characteristics can help businesses tailor their marketing strategies to each group's specific needs and preferences.\n",
      "\n",
      "10. Customer Retention: It is often more cost-effective to retain existing customers than to acquire new ones. Focusing on customer retention strategies such as excellent service, loyalty programs, and personalized communications can help increase customer lifetime value and overall profitability.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask your question based on the uploaded PDF (or type 'exit' to quit):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the conversation.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a5fe4-5e73-4749-be9a-720986a4322b",
   "metadata": {},
   "source": [
    "## Integration with Streamlit UI \n",
    "\n",
    "Run this cell to copy the entire code to a `.py` named `app.py`. Launch a new terminal an type `streamlit run app.py` to see the entire rag system demonstarted above with an interactive UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c15a2a-53d1-4730-8804-fa9f3eb9d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./app.py\n",
    "\n",
    "import streamlit as st\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def get_conversational_answer(retriever,input,chat_history):\n",
    "    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "    which might reference context in the chat history, formulate a standalone question \\\n",
    "    which can be understood without the chat history. Do NOT answer the question, \\\n",
    "    just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "    qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "    Use the following pieces of retrieved context to answer the question. \\\n",
    "    If you don't know the answer, just say that you don't know. \\\n",
    "    Use three sentences maximum and keep the answer concise.\\\n",
    "    Donot generate any additional text unless you are asked to.\\\n",
    "    Keep the answers really short and concise.\\\n",
    "\n",
    "    {context}\"\"\"\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    ai_msg = rag_chain.invoke({\"input\": input, \"chat_history\": chat_history})\n",
    "    return  ai_msg\n",
    "\n",
    "\n",
    "def main():\n",
    "    st.header('Chat with your PDF')\n",
    "    \n",
    "    if \"conversation\" not in st.session_state:\n",
    "        st.session_state.conversation = None\n",
    "\n",
    "    if \"activate_chat\" not in st.session_state:\n",
    "        st.session_state.activate_chat = False\n",
    "\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "        st.session_state.chat_history=[]\n",
    "\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"], avatar = message['avatar']):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    embed_model = OllamaEmbeddings(model='mistral')\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.subheader('Upload Your PDF File')\n",
    "        docs = st.file_uploader('Upload your PDF & Click to process',accept_multiple_files = True, type=['pdf'])\n",
    "        if st.button('Process'):\n",
    "            if docs is not None:\n",
    "                os.makedirs('./data', exist_ok=True)\n",
    "                for doc in docs:\n",
    "                    save_path = os.path.join('./data', doc.name)\n",
    "                    with open(save_path, 'wb') as f:\n",
    "                        f.write(doc.getbuffer())\n",
    "                    st.write(f'Processed file: {save_path}')\n",
    "           \n",
    "            with st.spinner('Processing'):\n",
    "                loader = PyPDFDirectoryLoader(\"./data\")\n",
    "                documents = loader.load()\n",
    "                vector_store = FAISS.from_documents(documents, embed_model)\n",
    "                retriever=vector_store.as_retriever()\n",
    "                if \"retriever\" not in st.session_state:\n",
    "                    st.session_state.retriever = retriever\n",
    "                st.session_state.activate_chat = True\n",
    "\n",
    "            # Delete uploaded PDF files after loading\n",
    "            for doc in os.listdir('./data'):\n",
    "                os.remove(os.path.join('./data', doc))\n",
    "\n",
    "    if st.session_state.activate_chat == True:\n",
    "        if prompt := st.chat_input(\"Ask your question based on the uploaded PDF\"):\n",
    "            with st.chat_message(\"user\", avatar = '👨🏻'):\n",
    "                st.markdown(prompt)\n",
    "            st.session_state.messages.append({\"role\": \"user\",  \"avatar\" :'👨🏻', \"content\": prompt})\n",
    "            retriever = st.session_state.retriever\n",
    "\n",
    "            ai_msg = asyncio.run(get_conversational_answer(retriever,prompt,st.session_state.chat_history))\n",
    "            st.session_state.chat_history.extend([HumanMessage(content=prompt), ai_msg[\"answer\"]])\n",
    "            cleaned_response=ai_msg[\"answer\"]\n",
    "            with st.chat_message(\"assistant\", avatar='🤖'):\n",
    "                st.markdown(cleaned_response)\n",
    "            st.session_state.messages.append({\"role\": \"assistant\",  \"avatar\" :'🤖', \"content\": cleaned_response})\n",
    "        else:\n",
    "            st.markdown('Upload your PDFs to chat')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
